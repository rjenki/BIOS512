{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.3.3"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"76e20893-a921-401f-bdf0-a5577a6e8171","cell_type":"markdown","source":"# Language Models\n**NEW** required R packages for this code (*already installed*): `httr`, `jsonlite`, `tokenizers`, `stringr`, `R6`, `digest`, and `viridis`\n\n## Why do we need `httr` and `jsonlite`?\n#### `httr`\nYour **web browser** is an object that makes requests over HTTP for content, documents, etc. It converts what you type in the address bar to an HTTP request.  \nA **language model** is a type of machine learning model that predicts the probability of a sequence of words to understand and generate human language, such as Chat GPT. All major language models have an HTTP API, which is a way for you to send HTTP requests to OpenAI from a program (*not a browser*) and to get responses from the language model.  \n`httr` is a library to **make HTTP requests and look at the results**.\n#### `jsonlite`\nYour web browser runs **JavaScript**, which is a programming language. Like most programming languages, it has ways of representing data, so lists and arrays and tables with named elements. **JSON** is basically a subset of JavaScript syntax that denotes data. If you're communicating with a *HTTP endpoint*, and you want to exchange data with it, rather than use it for a web page, those endpoints will almost always exchange that *data in the form of JSON*.  \n`jsonlite` is a library to **parse JSON into our objects**.\n\n## Back To Language Models\nA **language model** is a **probability distribution over words**. You put some input into the probability distribution, which is conditioning the probability distribution, and you get an output from the probability distribution. It's basically predicts the next word (*token*) given the previous words.\n\n### Markov Model\nA **Markov model** is a mathematical tool that predicts future states based on the current state, without considering the past history.","metadata":{}},{"id":"7fc841b0-713a-42a8-acf2-5a48ae7f1445","cell_type":"code","source":"# required libraries\nlibrary(httr)\nlibrary(tokenizers)\nlibrary(stringr)\n\n# Step 1. Fetch text from URL\nurl <- \"https://www.gutenberg.org/cache/epub/10662/pg10662.txt\" # Text file of The Night Land by William Hope Hodgson\nresp <- httr::GET(url)\ntext <- httr::content(resp, as = \"text\", encoding= \"UTF-8\")\n\n# Step 2. Tokenize the text\ntokens <- tokenizers::tokenize_words(text, lowercase=TRUE, strip_punct=TRUE)[[1]]\n\n# Step 3. Build the Markov model (bigram-based)\nmarkov_model <- new.env(parent = emptyenv())\n\nget_counts <- function(env, key) {\n    if (!is.null(env[[key]])) env[[key]] else {\n        x <- integer(0); names(x) <- character(0); x\n    }\n}\n\nif (length(tokens) > 1) {\n    for (i in seq_len(length(tokens)-1)) {\n        current_word <- tokens[i]\n        next_word <- tokens[i+1]\n        counts <- get_counts(markov_model, current_word)\n        if (next_word %in% names(counts)) {\n            counts[[next_word]] <- counts[[next_word]] + 1L\n        } else {\n            counts[[next_word]] <- 1L\n        }\n        markov_model[[current_word]] <- counts\n    }\n}\n\n# Step 4. Function to predict the next word\npredict_next_word <- function(word, model=markov_model) {\n    counts <- get_counts(model, word)\n    if (length(counts)==0) return(NA_character_)\n    sample(names(counts), size=1, prob=as.numeric(counts))\n}\n\n# Step 5. Generate a sequence\ngenerate_text <- function(start_word, length=10) {\n    word_sequence <- c(start_word)\n    for (i in seq_len(length - 1)) {\n        next_word <- predict_next_word(tail(word_sequence, 1))\n        word_sequence <- c(word_sequence, next_word)\n    }\n    paste(word_sequence, collapse= \" \")\n}\n\n# Example usage\nprint(generate_text(\"love\", 45))\nprint(generate_text(\"she\", 10))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"[1] \"love and now this purpose and brutish men as i set out mine arms a dull glowing of the end to a proper to make an illusion of my will of my head from out of my head piece of her quiet for after that\"\n[1] \"she to have read the diskos that we travelled over\"\n"}],"execution_count":1},{"id":"c2fcff67-1e43-4aab-b833-33c419ec1368","cell_type":"markdown","source":"Looking at these outputs, they kind of make sense if you look at the pairs, but don't make sense together. We only are including one word as the context, so **our model will improve if we give it more context**.  \n\nThe code below something called an **ngram**, which is the last n tokens.","metadata":{}},{"id":"067e8549-67fb-4fd8-8350-00ad6a5e273f","cell_type":"code","source":"library(httr)\nlibrary(tokenizers)\n\ntokenize_text <- function(text) {\n    tokenizers::tokenize_words(text, lowercase=TRUE, strip_punct=TRUE)[[1]]\n}\n\nkey_from <- function(ngram, sep = \"\\x1f\") {\n    paste(ngram, collapse=sep)\n}\n\nbuild_ngram_table <- function(tokens, n, sep = \"\\x1f\") {\n    if (length(tokens) < n) return(new.env(parent = emptyenv()))\n    tbl <- new.env(parent = emptyenv())\n    for (i in seq_len(length(tokens) - n + 1L)) {\n        ngram <- tokens[i:(i + n - 2L)]\n        next_word <- tokens[i + n - 1L]\n        key <- paste(ngram, collapse = sep)\n        counts <- if (!is.null(tbl[[key]])) tbl[[key]] else integer(0)\n        if (next_word %in% names(counts)) {\n            counts[[next_word]] <- counts[[next_word]] + 1L\n        } else {\n            counts[[next_word]] <- 1L\n        }\n        tbl[[key]] <- counts\n    }\n    tbl\n}\n\ndigest_text <- function(text, n) {\n    tokens <- tokenize_text(text)\n    build_ngram_table(tokens, n)\n}\n\ndigest_url <- function(url, n) {\n    res <- httr::GET(url)\n    txt <- httr::content(res, as = \"text\", encoding = \"UTF-8\")\n    digest_text(txt,n)\n}\n\nrandom_start <- function(tbl, sep = \"\\x1f\") {\n    keys <- ls(envir = tbl, all.names=TRUE)\n    if (length(keys)==0) stop(\"No n-grams available. Digest text first.\")\n    picked <- sample(keys, 1)\n    strsplit(picked, sep, fixed=TRUE)[[1]]\n}\n\npredict_next_word <- function(tbl, ngram, sep = \"\\x1f\") {\n    key <- paste(ngram, collapse = sep)\n    counts <- if(!is.null(tbl[[key]])) tbl[[key]] else integer(0)\n    if (length(counts) == 0) return(NA_character_)\n    sample(names(counts), size=1, prob=as.numeric(counts))\n}\n\nmake_ngram_generator <- function(tbl, n, sep = \"\\x1f\") {\n    force(tbl); n <- as.integer(n); force(sep)\n    function(start_words = NULL, length = 10L) {\n        if ((is.null(start_words)) || length(start_words) != n - 1L) {\n            start_words <- random_start(tbl, sep=sep)\n        }\n        word_sequence <- start_words\n        for (i in seq_len(max(0L, length - length(start_words)))) {\n            ngram <- tail(word_sequence, n - 1L)\n            next_word <- predict_next_word(tbl, ngram, sep=sep)\n            if (is.na(next_word)) break\n            word_sequence <- c(word_sequence, next_word)\n        }\n        paste(word_sequence, collapse= \" \")\n    }\n}\n\n# Using it (n=3)\nurl <- \"https://www.gutenberg.org/cache/epub/10662/pg10662.txt\"\ntbl3 <- digest_url(url, n=3)\ngen3 <- make_ngram_generator(tbl3, n=3)\n\nprint(gen3(length=128))\n\n# Using it (n=5)\nurl <- \"https://www.gutenberg.org/cache/epub/10662/pg10662.txt\"\ntbl5 <- digest_url(url, n=5)\ngen5 <- make_ngram_generator(tbl5, n=5)\n\nprint(gen5(length=128))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"[1] \"the refuge of humanity and surely it did be very sedate outward and to set you the working of her as i have no knowledge this way as i stood utter still was that i be not to be a lack because that my tales concerning the olden sea bed did be quiet as that eternity and to this side there was a great caution and we only to the front and so was she so utter happy and she took odd leave with her and surely it alway now to shake the aether half across the abyss of the light from the terror of the world the deep valley with redness so that in that it had been that the dead the diskos made a smooth place\"\n[1] \"that did go upward for ever into the everlasting night and so i did be an utter mystery and deathly dark beyond the shining of the morning sun and know it by name and the meaning of aught else and yet as i do strive to make plain unto you because that this thing must be and because that i went with a very wary hearing i heard the sound of it running very swiftly and coming nigh and it passed me and did go up the stream and there was surely a quick stupor upon me for i perceived that we did be come so near and the rock surely to fall backward into some deep pit whence came the upbursting of the water and so did\"\n"}],"execution_count":2},{"id":"615d7c13-a9c5-43c3-8a79-b311a1beb359","cell_type":"markdown","source":"### Can I just use bigger and bigger lookbacks to get a better language model?\nNo! You need balance. The more words you use in the ngrams, the less ngrams you have. At a centain point, you will only receive the input text back to you. We don't have enough training data to use a substantial amount of context.\n\n### Ways To Think of A Solution\n1. Track concepts instead of word. Assume that tokens in the past are not as interesting, so you only need to know their parts of speech or what type of object they are. But, if we're looking at Alice in Wonderland, we want to generate something on the White Rabbit, not a Colored Thing.\n2. Choose which details to pay attention to in the history of the text. Create a function that takes the current state of the model and returns the parts thar are relevant to generating the next token. We need a **function that can pay attention to the text contextually**.\n\nWith enough training and enough degrees of freedom, neural networks can do an adequate job of looking at an input text and generating a new token. This tracks the rough meaning of words but doesn't depend on specific words.\n\n### Attention Mechanism\nThe idea of how to solve our issue with language models lies in the attention mechanism of neural networks.  \n**Transformers** take our entire text as input and learn how to pull out words which are important to this word, and with this extra information,  we're able to produce a uniform representation that we can pass to a neural network.  \nThen, when we're training the neural network, we have an objective function and a gradient, and it learns how to do all this stuff.  \nThe general idea from the Markov model of taking previous words in your corpus, and calculating a probability distribution for the next word still applies. \n![](attn_mech.png)","metadata":{}},{"id":"f79aaf10-1211-432a-98b5-ec28b1ae4240","cell_type":"markdown","source":"When there's no training data that reflects the stuff that you're trying to ask the model, it cannot do the goal, because it doesn't have anything outside of the distribution.","metadata":{}},{"id":"7d77ea6e-41c6-4da3-bbd5-b5579408afa4","cell_type":"markdown","source":"## Next Section - Running Language Models Locally\nPlease see the file `running_lm_locally.Rmd`, download it, and follow the instructions to get OLLAMA installed. This section doesn't run on Binder (because it runs locally). If you just want to view it, you can see the HTML version!","metadata":{}}]}